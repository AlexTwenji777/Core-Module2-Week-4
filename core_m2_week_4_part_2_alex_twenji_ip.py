# -*- coding: utf-8 -*-
"""Core_M2_Week_4_Part_2_Alex_Twenji_IP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XNZCRlhAZ3txPnO6fa3mL9iPVRbmqkux

# DEFINING THE QUESTION

### a) Specifying the Question

This week's project requires us to implement Naive Bayes classifier and calculate the resulting metrics: We will be trying to Determine the Spam Email

---

## b) Defining the Metric for Success

Being able to accurately predict SPAMs.

---

## c) Understanding the context

The last column of 'spambase.data' denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail. Most of the attributes indicate whether a particular word or character was frequently occuring in the e-mail. The run-length attributes (55-57) measure the length of sequences of consecutive capital letters. Here are the definitions of the attributes:

48 continuous real [0,100] attributes of type word_freq_WORD
= percentage of words in the e-mail that match WORD, i.e. 100 * (number of times the WORD appears in the e-mail) / total number of words in e-mail. A "word" in this case is any string of alphanumeric characters bounded by non-alphanumeric characters or end-of-string.

6 continuous real [0,100] attributes of type char_freq_CHAR]
= percentage of characters in the e-mail that match CHAR, i.e. 100 * (number of CHAR occurences) / total characters in e-mail

1 continuous real [1,...] attribute of type capital_run_length_average
= average length of uninterrupted sequences of capital letters

1 continuous integer [1,...] attribute of type capital_run_length_longest
= length of longest uninterrupted sequence of capital letters

1 continuous integer [1,...] attribute of type capital_run_length_total
= sum of length of uninterrupted sequences of capital letters
= total number of capital letters in the e-mail

1 nominal {0,1} class attribute of type spam
= denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail.

---

## d) Experimental Design
1. Read and explore the given dataset.

2. Find and deal with outliers, anomalies, and missing data within the dataset.

3. Perform Exploratory Data Analysis.

4. Perforn Naive Bayes Classification.

5. Provide a recommendation based on your analysis.

6. Challenge your solution by providing insights on how you can make improvements in model improvement.

---

## e) Data Relevance

The "spam" concept is diverse: advertisements for products/web sites, make money fast schemes, chain letters, pornography...

This collection of spam e-mails came from the collector's postmaster and individuals who had filed spam. The collection of non-spam e-mails came from filed work and personal e-mails, and hence the word 'george' and the area code '650' are indicators of non-spam. These are useful when constructing a personalized spam filter. One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter.

---

# DATA PREPARATION
"""

import pandas as pd
import numpy as np

pd.read_csv('/content/spambase.data', header=None)

pd.read_csv('/content/spambase.names', sep='delimiter', header= None)

pd.read_csv('/content/spambase.DOCUMENTATION', sep='delimiter', header= None)

# Alternative reading of data

!wget https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data

group_name = 'spam'

# Loads the CSV data
df = pd.read_csv('spambase.data', header=None)
df

df.head()

df.tail()

df.shape

df.info()

"""# DATA CLEANING"""

df.isnull().sum()

"""The Dataset is clean

---

# DATA ANALYSIS
"""

df.corr()

"""# MODELING

## PREPARATION
"""

from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

X = df.loc[:, 0:56]
y = df.loc[:,57]

"""## 80:20 Model """

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.20, random_state= 10)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

gaus = GaussianNB()

gaus.fit(X_train, y_train)

y_pred = gaus.predict(X_test)

print('Accuracy Score is: ', accuracy_score(y_pred, y_test))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""## 70:30 Model"""

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.30, random_state= 10)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

gaus = GaussianNB()

gaus.fit(X_train, y_train)

y_pred = gaus.predict(X_test)

print('Accuracy Score is: ', accuracy_score(y_pred, y_test))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""## 60:40 Model"""

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.40, random_state= 10)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

gaus = GaussianNB()

gaus.fit(X_train, y_train)

y_pred = gaus.predict(X_test)

print('Accuracy Score is: ', accuracy_score(y_pred, y_test))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""We can notice that as the train split decreases the accuracy scores decrease, indicating the more the data for training, the better the results will be. Therefore, our 80:20 Model split gave the best results.

## OPTIMIZING 80:20 Model
"""

corr_matrix = X.corr().abs()

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Find index of feature columns with correlation greater than 0.80
to_drop = [column for column in upper.columns if any(upper[column] > 0.80)]

# Drop features 
X.drop(X[to_drop], axis=1)

# Seems that no columns are highly correlated

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.20, random_state= 10)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

# To try improve the scores, we could try and improve the imbalance of the classes as
# indicated by the lower support values of class 1 in all models.

# import library
from imblearn.over_sampling import RandomOverSampler
from collections import Counter

ros = RandomOverSampler(random_state=42)

# fit predictor and target variablex_ros, 

x_ros, y_ros = ros.fit_resample(X, y)

print('Original dataset shape', Counter(y))
print('Resample dataset shape', Counter(y_ros))

X_train, X_test, y_train, y_test = train_test_split(x_ros, y_ros, test_size=0.20, random_state= 10)

scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

gaus = GaussianNB()

gaus.fit(X_train, y_train)

y_pred = gaus.predict(X_test)

print('Accuracy Score is: ', accuracy_score(y_pred, y_test))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# The f1- scores have slightly improved, possibly due to the improved class balance as shown by 
# the support values

"""# CONCLUSION

The precision informs us on the accuracy of the true positive predictions with regards to false positives. The recall informs us on the accuracy of the true positive predictions with regards to false negatives. The f1-score finds the best balance between precision and recall. For this challenge the best accuracy score to work with, in order to beat the accuracy paradox of the accuracy score is the f1-score. Using this, the 80:20 split Model is the best Model.

---

# CHALLENGING THE SOLUTION
"""

# Using Multinomial

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1, random_state=69)

model = MultinomialNB()
model.fit(X_train, y_train)

from sklearn.model_selection import GridSearchCV

alpha = np.logspace(0,10,20)
hyperparams = dict(alpha = alpha)

clf = GridSearchCV(model, hyperparams, cv=5)

best_model = clf.fit(X_train, y_train)

# Viewing best hyperparameters
print('Best alpha:', best_model.best_estimator_.get_params()['alpha'])
print('Best Class_prior:', best_model.best_estimator_.get_params()['class_prior'])
print('Best Fit_prior:', best_model.best_estimator_.get_params()['fit_prior'])

# the best hyperparameters are similar to the basic model.

y_pred = model.predict(X_test)

print('Accuracy Score is: ', accuracy_score(y_pred, y_test))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# The values are lower, showing that our best model was the Gauss NB.

"""# RECOMMENDATION

Our Gaussian method gave the best better f1-score values. The dataset however should have been provided with better column names in order to know what each feature represents.

---

# FOLLOW UP QUESTIONS

## a) Did we have the right data?

No, we do not know what all the features represent

## b) Do we need other data to answer our question?

Yes, we need column names in order to know what each feature represents.

## c) Did we have the right question?

Yes, because SPAM emails have led to many scams.
"""